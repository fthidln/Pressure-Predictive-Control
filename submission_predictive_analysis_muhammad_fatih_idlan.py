# -*- coding: utf-8 -*-
"""Submission_Predictive Analysis_Muhammad Fatih Idlan

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16h-BCzHy98W51yElfnJeiz4ezqdaHsCc

# **Predictive Analysis: Optimizing Pressure Control through Machine Learning**

By    : Muhammad Fatih Idlan (faiti.alfaqar@gmail.com)

# Project Domain
This project was done to fulfil the *Machine Learning Terapan* 1st assignment submission on Dicoding. The domain used in this project is manufacturing control process, especially pressure control.

# Background
Pressure control is a fundamental aspect of many industrial processes, particularly in chemical engineering, where maintaining optimal pressure levels can significantly enhance efficiency, safety, and product quality. However, real-time fluctuations due to varying input conditions, system disturbances, and equipment aging pose challenges to traditional control methods. Traditional pressure control methods rely on Proportional-Integral-Derivative (PID) controllers, which require manual tuning and often struggle with dynamic system behaviors or process disturbances. Moreover, increasing feedback noise making PID performs poorly comparing to neural network model [[ 1 ]](https://www.semanticscholar.org/paper/A-comparison-between-a-traditional-PID-controller-a-Conradt/efb1c57c0dbc3b88cd35085f677869104fce5474). The existence of feedback noise is inevitably present in real world setting. Thus making machine learning based model is more flexible in real-time fluctuations.

# Business Understanding
## Problem Statement
Starting with explanation from the background above, core problems that this project aims to solve are:

*   What are the variables that hugely affect target i.e. source pressure for developing predictive models that dynamically adjust it?
*   How are the variables those hugely affect the source pressure is related?
*   How the performance of each model to predict the source pressure that has been build?

## Objectives
According to problem statement above, this project has several objectives too, that are:

*   Knowing the most influencial variables toward source pressure in the system
*   Learn the relation between influencial variables to source pressure
*   Determining high performance models

## Solution
To achive the objectives, we need to perform several things such as:

*   Implementing correlation heatmap for each variables to identify influencial variables
*   Using Linear Regression, K-Nearest Neighbour, and Dense Neural Network to selecting high performance corresponding to evaluation metrics (MSE)

# Import Package dan Libraries
"""

import kagglehub
import shutil
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.neighbors import KNeighborsRegressor
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor
from lightgbm import LGBMRegressor
from sklearn.model_selection import GridSearchCV

"""# Data Loading"""

path = kagglehub.dataset_download("guanlintao/smart-pressure-control-prediction")

print("Path to dataset files:", path)

train_path = f'{path}/train.csv'
test_path = f'{path}/test.csv'

# Merge the train and test set from kaggle

train = pd.read_csv(train_path)
test = pd.read_csv(test_path)
merge = pd.concat([train, test])
merge.info()

"""## Data Cleaning

*   Checking empty data
*   Checking duplicated data


"""

print(merge.isna().sum())

print(f'Duplicated data: {merge.duplicated().sum()}')

# Remove duplicated data

merge_1 = merge.drop_duplicates()
print(f'Duplicated data: {merge_1.duplicated().sum()}')

merge_1.info()

merge_1.head()

"""# Data Understanding

The dataset that used in this project is Smart Pressure Control Prediction, which can be accessed through kaggle [[ 2 ]](https://www.kaggle.com/datasets/guanlintao/smart-pressure-control-prediction). This dataset consist of 2 csv files, train and test, which in total has 4320 rows with 32 column. The explanation for each column can be seen below:

*   DEGC1PV = Equipment temperature in zone 1
*   DEGC2PV = Equipment temperature in zone 2
*   DEGC3PV = Equipment temperature in zone 3
*   DEGC4PV = Equipment temperature in zone 4
*   DEGC5PV = Equipment temperature in zone 5
*   DEGC6PV = Equipment temperature in zone 6
*   DEGC1SV = Desired equipment temperature in zone 1
*   DEGC2SV = Desired equipment temperature in zone 2
*   DEGC3SV = Desired equipment temperature in zone 3
*   DEGC4SV = Desired equipment temperature in zone 4
*   DEGC5SV = Desired equipment temperature in zone 5
*   DEGC6SV = Desired equipment temperature in zone 6
*   NM3/H.1PV = Air flowrate in zone 1
*   NM3/H.2PV = Air flowrate in zone 2
*   NM3/H.3PV = Air flowrate in zone 3
*   NM3/H.4PV = Air flowrate in zone 4
*   NM3/H.5PV = Air flowrate in zone 5
*   NM3/H.6PV = Air flowrate in zone 6
*   NM3/H.1SV = Desired air flowrate in zone 1
*   NM3/H.2SV = Desired air flowrate in zone 2
*   NM3/H.3SV = Desired air flowrate in zone 3
*   NM3/H.4SV = Desired air flowrate in zone 4
*   NM3/H.5SV = Desired air flowrate in zone 5
*   NM3/H.6SV = Desired air flowrate in zone 6
*   TEMP = Air temperature
*   FC1 = Control valve opening degree in zone 1
*   FC2 = Control valve opening degree in zone 2
*   FC3 = Control valve opening degree in zone 3
*   FC4 = Control valve opening degree in zone 4
*   FC5 = Control valve opening degree in zone 5
*   FC6 = Control valve opening degree in zone 6
*   mmH2O = Source input pressure

# Exploratory Data Analysis (EDA)

## Statistical Properties
"""

merge_1.describe()

"""## Data Outlier"""

for i in merge_1.columns:
  sns.boxplot(x=merge_1[i])
  plt.show()



"""## Multivariate Analysis
### Correlation Matrix
"""

num_val = merge_1.select_dtypes(include=[np.number]).corr().round(2)

plt.figure(figsize=(15, 15))
sns.heatmap(num_val, annot=True, linewidths=0.5, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

unrelated_column = ['DEGC1SV', 'DEGC2SV', 'DEGC3SV', 'DEGC4SV', 'DEGC5SV', 'DEGC6SV', 'TEMP', 'FC4', 'FC5', 'FC6',
                    'DEGC1PV', 'DEGC2PV', 'DEGC3PV', 'DEGC4PV', 'DEGC5PV', 'DEGC6PV', 'NM3/H.3PV', 'NM3/H.4PV',
                    'NM3/H.5PV', 'NM3/H.6PV', 'NM3/H.3SV', 'NM3/H.4SV', 'NM3/H.5SV', 'NM3/H.6SV', 'FC1', 'FC2', 'FC3']

merge_2 = merge_1.copy()

for i in unrelated_column:
  try:
    merge_2 = merge_2.drop(i, axis=1, errors='ignore')
  except:
    pass

merge_2.info()

sns.pairplot(merge_2)

"""### Important Key Points

*   All DEGC2SV variable values are stagnant at 1070, so they have no impact on the target
*   Each variable has quite a lot of outlier values, but it is still retained because it can represent noise in real time
*   From correlation matrix above, we can conclude that NM3/H.1PV, NM3/H.2PV, NM3/H.1SV, and NM3/H.2SV is the most influencial variables to source input pressure, so we can drop the other unnacessary variables

## Principal Component Analysis
This step is important, Principal Component Analysis (PCA) helps to eliminate redundancy by transforming the original features into a smaller set of uncorrelated variables (principal components), making the data easier to analyze by the model. Turns out that the most influencial principal component variance is 0.978, followed by 0.012 and 0.009. We can ignore the last two dimension because it has a very small variance corresponding to the first one [[ 3 ]](https://www.sciencedirect.com/science/article/pii/S1877050919321507). Thus simplify the problem that the models try to solve [[ 4 ]](https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0202).
"""

pca = PCA(n_components=3, random_state=37)
pca_result = pca.fit(merge_2[['NM3/H.1PV', 'NM3/H.2PV', 'NM3/H.1SV', 'NM3/H.2SV']])
princ_comp = pca.transform(merge_2[['NM3/H.1PV', 'NM3/H.2PV', 'NM3/H.1SV', 'NM3/H.2SV']])

pca.explained_variance_ratio_.round(3)

merge_2.info()

pca = PCA(n_components=1, random_state=37)
pca_result = pca.fit(merge_2[['NM3/H.1PV', 'NM3/H.2PV', 'NM3/H.1SV', 'NM3/H.2SV']])
merge_2['dimension'] = pca.transform(merge_2.loc[:, ('NM3/H.1PV', 'NM3/H.2PV', 'NM3/H.1SV', 'NM3/H.2SV')]).flatten()
merge_2.drop(['NM3/H.1PV', 'NM3/H.2PV', 'NM3/H.1SV', 'NM3/H.2SV'], axis=1, inplace=True)
merge_2.head()

merge_2.head()

"""# Spliting Dataset into Train and Test Set
To initiate the model development, splitting the data into train and test set is necessary. Moreover, this project using supervised learning. The train set serve as learning agent while test set serve as evaluating agent.
"""

x = merge_2[['dimension']]
y = merge_2['mmH2O']
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 37)

print(f'Total # of sample in whole dataset: {len(x)}')
print(f'Total # of sample in train dataset: {len(x_train)}')
print(f'Total # of sample in test dataset: {len(x_test)}')

"""## Standardization
In order to scaling the dataset value, we can use standardization method. It transform the dataset in such a way to have a mean of 0 and standard deviation of 1. Moreover, standardization method is the superior scaling technique for medium and large dataset [[ 5 ]](https://ieeexplore.ieee.org/document/10681438).
"""

scaler = StandardScaler()
scaler.fit(x_train)
x_train = scaler.transform(x_train)

scaler.fit(x_test)
x_test = scaler.transform(x_test)

"""# Model Development
In this step, the algorithm used for model developments are K-Nearest Neighbour, Linear Regression, and Dense Neural Network.

* K-Nearest Neighbour = KNN is a simple, instance-based learning algorithm. It classifies a new data point based on the majority class of its K-nearest neighbors in the feature space.
 * Pros
   * Simple to understand and implement
   * No explicit training phase (lazy learning)
 * Cons
   * Computationally expensive for large datasets (due to distance calculations)
   * Sensitive to irrelevant or unscaled features
   * Performance depends on the choice of K and distance metric
* Linear Regression = Linear regression models the relationship between a dependent variable (target) and one or more independent variables (features) by fitting a linear equation to the data.
 * Pros
   * Simple, interpretable model
   * Works well when there is a linear relationship between features and the target
 * Cons
   * Limited to linear relationships
   * Sensitive to outliers
   * Assumes no multicollinearity between features (when using multiple features)
* Dense Neural Network = A dense neural network (DNN) consists of layers of neurons where each neuron in one layer is connected to every neuron in the next layer (hence the term "fully connected").
 * Pros
   * Can model highly complex relationships between input and output
   * Scalable to large datasets and tasks like image recognition, natural language processing, etc
 * Cons
   * Requires a large amount of data and computational resources to train effectively
   * Prone to overfitting, especially with small datasets
   * Difficult to interpret compared to simpler models like linear regression
"""

models = pd.DataFrame(index=['train_mse', 'test_mse'],
                      columns=['KNN','Linear Regression', 'ANN'])

LR = LinearRegression()
LR.fit(x_train, y_train)

models.loc['train_mse','Linear Regression'] = mean_squared_error(y_pred=LR.predict(x_train), y_true=y_train)

model = Sequential()
model.add(Dense(2048, input_dim=1, activation='relu'))
model.add(Dense(1024, activation='relu'))
model.add(Dense(512, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(1, activation='linear'))

model.compile(loss='mse', optimizer='adam', metrics=['mse'])

model.fit(x_train, y_train, epochs=500, batch_size=100, verbose=2)

models.loc['train_mse','ANN'] = mean_squared_error(y_pred=model.predict(x_train), y_true=y_train)

KNN = KNeighborsRegressor(n_neighbors=5, algorithm='brute')
KNN.fit(x_train, y_train)

models.loc['train_mse','KNN'] = mean_squared_error(y_pred=KNN.predict(x_train), y_true=y_train)

models

"""From metric evaluation table above, we can conclude that K-Nearest Neighbour algorithm is the most desired algortihm because has the lowest MSE value, followed by Dense Neural Network, and the last is Linear Regression.

# Model Evaluation
The matrix evaluation used for this step is Mean Squared Error
\begin{align*}
\text{MSE}(y, x) = \frac{\sum_{i=0}^{N - 1} (y_i - x_i)^2}{N}
\end{align*}

Where:

* N = Amount of the data
* i = Index of the data
* y = Actual value
* x = Predicted value

MSE is a metric used to measure the average squared difference between the predicted values and the actual values in the dataset. It is calculated by taking the average of the squared residuals, where the residual is the difference between predicted value and the actual value for each data point [[ 6 ]](https://www.geeksforgeeks.org/mean-squared-error/). A lower MSE indicates that the model's predictions are closer to the actual values signifying better accuracy. While, a higher MSE suggests that the model's predictions deviate further from true values indicating the poorer performance.
"""

mse = pd.DataFrame(columns=['train', 'test'], index=['KNN','Linear Regression', 'ANN'])

model_dict = {'KNN': KNN, 'Linear Regression': LR, 'ANN': model}

for name, model in model_dict.items():
    mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(x_train))
    mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(x_test))

mse

fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

x_train = pd.DataFrame(x_train, columns=['dimension'])
x_test = pd.DataFrame(x_test, columns=['dimension'])

"""## Model Prediction
This step is carried out to see how each machine learning algorithm predicting the target data (source pressure).
"""

pred = x_test.iloc[:500].copy()

pred_dict = {'y_true': y_test.iloc[:500]}
pred_dict['dimension'] = pred['dimension'].values
pred_dict['LR'] = LR.predict(pred).ravel()
pred_dict['KNN'] = KNN.predict(pred).ravel()
pred_dict['ANN'] = model.predict(pred).ravel()

pd.DataFrame(pred_dict)

plt.subplots(figsize=(12, 12))

plt.subplot(311)
plt.plot(pred_dict['dimension'], pred_dict['y_true'], 'o', label='Real data')
plt.plot(pred_dict['dimension'], pred_dict['LR'], 'o', label='Linear Regression')
plt.ylabel('Pressure [mmH2O]')
plt.tick_params('x', labelbottom=False)
plt.legend()

plt.subplot(312)
plt.plot(pred_dict['dimension'], pred_dict['y_true'], 'o', label='Real data')
plt.plot(pred_dict['dimension'], pred_dict['KNN'], 'o', label='KNN')
plt.tick_params('x', labelbottom=False)
plt.legend()

plt.subplot(313)
plt.plot(pred_dict['dimension'], pred_dict['y_true'], 'o', label='Real data')
plt.plot(pred_dict['dimension'], pred_dict['ANN'], 'o', label='ANN')
plt.legend()
plt.xlabel('Dimension')

plt.show()

"""From the figure above, we can compare how prediction data and real data from each machine learning algorithm (K-Nearest Neighbour, Linear Regression, Dense Neural Network). Clearly, Linear Regression generated data point in a straight line. K-Nearest Neighbour generated data points that gather in one area. Then Dense Neural Network seems to struggle with its predictions forming a smoother but lower curve that doesn't capture the wide spread of real data.

# Reference

*   [ 1 ] J. Conradt, “A comparison between a traditional PID controller and an Artificial Neural Network controller in manipulating a robotic arm,” 2019. Accessed: Oct. 22, 2024. [Online]. Available: https://www.semanticscholar.org/paper/A-comparison-between-a-traditional-PID-controller-a-Conradt/efb1c57c0dbc3b88cd35085f677869104fce5474

*   [ 2 ] “Smart Pressure Control Prediction.” Accessed: Oct. 23, 2024. [Online]. Available: https://www.kaggle.com/datasets/guanlintao/smart-pressure-control-prediction

*   [ 3 ] N. Salem and S. Hussein, “Data dimensional reduction and principal components analysis,” Procedia Computer Science, vol. 163, pp. 292–299, Jan. 2019, doi: 10.1016/j.procs.2019.12.111.

*   [ 4 ] I. T. Jolliffe and J. Cadima, “Principal component analysis: a review and recent developments,” Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, vol. 374, no. 2065, p. 20150202, Apr. 2016, doi: 10.1098/rsta.2015.0202.

*   [ 5 ] K. Mahmud Sujon, R. Binti Hassan, Z. Tusnia Towshi, M. A. Othman, M. Abdus Samad, and K. Choi, “When to Use Standardization and Normalization: Empirical Evidence From Machine Learning Models and XAI,” IEEE Access, vol. 12, pp. 135300–135314, 2024, doi: 10.1109/ACCESS.2024.3462434.

*   [ 6 ] “Mean Squared Error | Definition, Formula, Interpretation and Examples,” GeeksforGeeks. Accessed: Oct. 23, 2024. [Online]. Available: https://www.geeksforgeeks.org/mean-squared-error/
"""